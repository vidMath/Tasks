{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Code that results average throughput per time, for each ‘Co Server’ and ASN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nSaved the given data into a data file and extracted into a rdd.\\nFrom the rdd created 3 more RDDs in key,value pair as below\\n- co-server, time  \\n- asn,time \\n- (co-server,asn),time\\nApllied combineByKey on both rdd and saved it into a list.\\nWrote the lists to 3 separate files\\n- ASN.data\\n- Co-server.data\\n- Combined.txt\\n'"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Saved the given data into a data file and extracted into a rdd.\n",
    "From the rdd created 3 more RDDs in key,value pair as below\n",
    "- co-server, time  \n",
    "- asn,time \n",
    "- (co-server,asn),time\n",
    "Apllied combineByKey on both rdd and saved it into a list.\n",
    "Wrote the lists to 3 separate files\n",
    "- ASN.data\n",
    "- Co-server.data\n",
    "- Combined.txt\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc = SparkContext.getOrCreate()\n",
    "mydata = sc.textFile(\"BigData.data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'de 31334 [07/Aug/2015:20:30:01 +0000] 76410 302 435 0.326',\n",
       " u'pl 50231 [07/Aug/2015:20:30:01 +0000] 126746 200 7400 0.000',\n",
       " u'gr 3329 [07/Aug/2015:20:30:01 +0000] 126474 206 17711 0.000',\n",
       " u'tr 9121 [07/Aug/2015:20:30:02 +0000] 76406 200 19589 0.000',\n",
       " u'se 3301 [07/Aug/2015:20:30:02 +0000] 76406 200 17960 0.000']"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mydata.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd1 = mydata.map(lambda x:x.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[u'de', u'31334', u'[07/Aug/2015:20:30:01', u'+0000]', u'76410', u'302', u'435', u'0.326'], [u'pl', u'50231', u'[07/Aug/2015:20:30:01', u'+0000]', u'126746', u'200', u'7400', u'0.000'], [u'gr', u'3329', u'[07/Aug/2015:20:30:01', u'+0000]', u'126474', u'206', u'17711', u'0.000'], [u'tr', u'9121', u'[07/Aug/2015:20:30:02', u'+0000]', u'76406', u'200', u'19589', u'0.000'], [u'se', u'3301', u'[07/Aug/2015:20:30:02', u'+0000]', u'76406', u'200', u'17960', u'0.000']]\n"
     ]
    }
   ],
   "source": [
    "print rdd1.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rdd2 = rdd1.map(lambda x : [x[1],x[5],float(x[7])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'31334', u'302', 0.326],\n",
       " [u'50231', u'200', 0.0],\n",
       " [u'3329', u'206', 0.0],\n",
       " [u'9121', u'200', 0.0],\n",
       " [u'3301', u'200', 0.0],\n",
       " [u'9198', u'200', 0.0],\n",
       " [u'6830', u'200', 0.0],\n",
       " [u'21246', u'200', 0.0],\n",
       " [u'21246', u'200', 0.0],\n",
       " [u'9050', u'206', 0.504],\n",
       " [u'8447', u'200', 0.006],\n",
       " [u'3320', u'200', 2.892],\n",
       " [u'3320', u'200', 0.007],\n",
       " [u'3320', u'200', 2.17],\n",
       " [u'8447', u'200', 0.029],\n",
       " [u'8447', u'200', 0.0],\n",
       " [u'1901', u'206', 0.47],\n",
       " [u'3320', u'200', 0.633],\n",
       " [u'12635', u'206', 0.0],\n",
       " [u'15685', u'206', 0.756]]"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[u'31334', u'302', 0.326], [u'50231', u'200', 0.0], [u'3329', u'206', 0.0], [u'9121', u'200', 0.0], [u'3301', u'200', 0.0], [u'9198', u'200', 0.0], [u'6830', u'200', 0.0], [u'21246', u'200', 0.0], [u'21246', u'200', 0.0], [u'9050', u'206', 0.504], [u'8447', u'200', 0.006], [u'3320', u'200', 2.892], [u'3320', u'200', 0.007], [u'3320', u'200', 2.17], [u'8447', u'200', 0.029], [u'8447', u'200', 0.0], [u'1901', u'206', 0.47], [u'3320', u'200', 0.633], [u'12635', u'206', 0.0], [u'15685', u'206', 0.756]]\n"
     ]
    }
   ],
   "source": [
    "l = rdd2.take(20)\n",
    "print l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'3320', 1.4255), (u'15685', 0.756), (u'21246', 0.0), (u'12635', 0.0), (u'9050', 0.504), (u'50231', 0.0), (u'9121', 0.0), (u'6830', 0.0), (u'31334', 0.326), (u'8447', 0.011666666666666667), (u'3329', 0.0), (u'9198', 0.0), (u'1901', 0.47), (u'3301', 0.0)]\n"
     ]
    }
   ],
   "source": [
    "'''dict_asn = []\n",
    "for el in l:\n",
    "    b = (el[0], el[2])\n",
    "    dict_asn.append(b)\n",
    "#print dict_asn\n",
    "\n",
    "rdd3 = sc.parallelize(dict_asn)\n",
    "rdd3.aggregateByKey(0,lambda acc,val: acc+val, lambda acc1, acc2: acc1+acc2).collect()'''\n",
    "rdd3 = rdd2.map(lambda x : (x[0],x[2]))\n",
    "#print rdd3.collect()\n",
    "#rdd3.aggregateByKey(0,lambda acc,val: acc+val, lambda acc1, acc2: acc1+acc2).collect()\n",
    "sumCount = rdd3.combineByKey(lambda value: (value, 1),\n",
    "                             lambda x, value: (x[0] + value, x[1] + 1),\n",
    "                             lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    "\n",
    "averageByKey = sumCount.map(lambda (label, (value_sum, count)): (label, value_sum / count))\n",
    "\n",
    "list_rdd_asn = averageByKey.collect()\n",
    "print list_rdd_asn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('ASN.data', 'w') as fp:\n",
    "    for x in list_rdd_asn:\n",
    "        #print x\n",
    "        fp.write(','.join(str(z) for z in x))\n",
    "        fp.write('\\n')\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'302', 0.326), (u'200', 0.4097857142857143), (u'206', 0.346)]\n"
     ]
    }
   ],
   "source": [
    "'''dict_co_server = []\n",
    "count_co_server={}\n",
    "for el in l:\n",
    "    b = (el[1], el[2])\n",
    "    count_co_server[el[1]]=sum(x.count(el[0]) for x in l)\n",
    "    dict_co_server.append(b)\n",
    "#print dict_co_server\n",
    "#print count\n",
    "rdd4 = sc.parallelize(dict_co_server)\n",
    "rdd4.aggregateByKey(0,lambda acc,val: acc+val, lambda acc1, acc2: acc1+acc2).collect()'''\n",
    "rdd4 = rdd2.map(lambda x : (x[1],x[2]))\n",
    "rdd4.take(20)\n",
    "#rdd4.aggregateByKey(0,lambda acc,val: acc+val, lambda acc1, acc2: acc1+acc2).collect()\n",
    "\n",
    "sumCount = rdd4.combineByKey(lambda value: (value, 1),\n",
    "                             lambda x, value: (x[0] + value, x[1] + 1),\n",
    "                             lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    "\n",
    "averageByKey = sumCount.map(lambda (label, (value_sum, count)): (label, value_sum / count))\n",
    "\n",
    "list_rdd_coserver = averageByKey.collect()\n",
    "print list_rdd_coserver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('Co-server.data', 'w') as fp:\n",
    "    for x in list_rdd_coserver:\n",
    "        #print x\n",
    "        fp.write(','.join(str(z) for z in x))\n",
    "        fp.write('\\n')\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[((u'1901', u'206'), 0), ((u'6830', u'200'), 0), ((u'3329', u'206'), 0), ((u'9198', u'200'), 0), ((u'8447', u'200'), 0), ((u'9121', u'200'), 0), ((u'3301', u'200'), 0), ((u'3320', u'200'), 4), ((u'50231', u'200'), 0), ((u'9050', u'206'), 0), ((u'15685', u'206'), 0), ((u'21246', u'200'), 0), ((u'12635', u'206'), 0), ((u'31334', u'302'), 0)]\n"
     ]
    }
   ],
   "source": [
    "rdd_combined = rdd2.map(lambda x : ((x[0],x[1]),int(x[2])))\n",
    "#print rdd_combined.take(20)\n",
    "list_rdd_combined = rdd_combined.aggregateByKey(0,lambda acc,val: acc+val, lambda acc1, acc2: acc1+acc2).collect()\n",
    "print list_rdd_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('Combined.txt', 'w') as fp:\n",
    "    for x in list_rdd_combined:\n",
    "        #print x[-1]\n",
    "        #for y in x:\n",
    "        #    print y\n",
    "        fp.write(','.join(str(z) for z in x[0]))\n",
    "        fp.write(',')\n",
    "        fp.write(''.join(str(x[-1])))\n",
    "        fp.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nI here grouped based on co-server and asn combined for multiple key grouping.\\nWe can group based on metric and country to get more features out of the data collected.\\nMetric will help in getting more insight into the data we are receiving.\\n'"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "I here grouped based on co-server and asn combined for multiple key grouping.\n",
    "We can group based on metric and country to get more features out of the data collected.\n",
    "Metric will help in getting more insight into the data we are receiving.\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
